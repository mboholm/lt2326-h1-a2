{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 for Assignment 2, LT2326, Autumn 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that part 1 and part 2 have been done in reverse order.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**: Max Boholm (gusbohom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB collected all libraries at one place\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.uniform import Uniform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB. New cell\n",
    "gpu_device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data, padding (based on 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start-of-sentence (sos) and end-of-sentence (eos) tokens have been added to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB added code for adding start-of-sentence (sos) and end-of-sentence (eos) tokens\n",
    "\n",
    "sos = \"#\"\n",
    "eos = \"!\"\n",
    "# MB. Neither \"#\" nor \"!\" seems to be in the original data\n",
    "\n",
    "def read_chinese_data(inputfilename):\n",
    "    with open(inputfilename, \"r\") as inputfile:\n",
    "        sentences = []\n",
    "        collection_words = []\n",
    "        collection_labels = []\n",
    "        for line in inputfile:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            columns = line.split()\n",
    "            if columns == []:\n",
    "                collection_words = [sos] + collection_words + [eos] # MB modified the iteration here\n",
    "                collection_labels = [1] + collection_labels + [1]   # ... and here\n",
    "\n",
    "                sentences.append((''.join(collection_words), collection_labels))\n",
    "                collection_words = []\n",
    "                collection_labels = []\n",
    "                continue\n",
    "            \n",
    "            collection_words.append(columns[1])\n",
    "            collection_labels += [1] + ([0] * (len(columns[1]) - 1))\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_chinese_data('/scratch/lt2316-h20-resources/zh_gsd-ud-train.conllu')\n",
    "# train_sentences[0] # MB added this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = read_chinese_data('/scratch/lt2316-h20-resources/zh_gsd-ud-test.conllu')\n",
    "# test_sentences[0] # MB added this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_chars(sentences):\n",
    "    megasentence = ''.join(sentences)\n",
    "    char_list = set()\n",
    "    for c in megasentence:\n",
    "        char_list.add(c)\n",
    "    char_list = [0] + list(char_list)\n",
    "    return char_list, {char_list[x]:x for x in range(len(char_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_index, char_index = index_chars([x[0] for x in train_sentences + test_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(sentence, index):\n",
    "    return [index[x] for x in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_lengths(sentences, max_length, padding=0):\n",
    "    return [x + ([padding] * (max_length - len(x))) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x, device=\"cpu\"):\n",
    "    converted = [(convert_sentence(x1[0], char_index), x1[1]) for x1 in x]\n",
    "    X, y = zip(*converted)\n",
    "    lengths = [len(x2) for x2 in X]\n",
    "    padded_X = pad_lengths(X, max(lengths))\n",
    "    Xt = torch.LongTensor(padded_X).to(device)\n",
    "    padded_y = pad_lengths(y, max(lengths), padding=-1)\n",
    "    yt = torch.LongTensor(padded_y).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    return Xt, lengths_t, yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor, train_lengths_tensor, train_y_tensor = create_dataset(train_sentences, gpu_device)\n",
    "test_X_tensor, test_lengths_tensor, test_y_tensor = create_dataset(test_sentences, gpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Packing the sequences for RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells for illstrating structure of data and performance of `pack_padded_sequence` and `pad_packed_sequence` have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batching (based on 1.0, 1.1, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells for illustrating `Batcher` have been removed. I have not done anything with `Batcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, X, lengths, y, device, batch_size=50, max_iter=None):\n",
    "        self.X = X\n",
    "        self.lengths = lengths # We need the lengths to efficiently use the padding.\n",
    "        self.y = y\n",
    "        self.device = device\n",
    "        self.batch_size=batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.curr_iter = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.curr_iter == self.max_iter:\n",
    "            raise StopIteration\n",
    "        permutation = torch.randperm(self.X.size()[0], device=self.device)\n",
    "        permX = self.X[permutation]\n",
    "        permlengths = self.lengths[permutation]\n",
    "        permy = self.y[permutation]\n",
    "        splitX = torch.split(permX, self.batch_size)\n",
    "        splitlengths = torch.split(permlengths, self.batch_size)\n",
    "        splity = torch.split(permy, self.batch_size)\n",
    "        \n",
    "        self.curr_iter += 1\n",
    "        return zip(splitX, splitlengths, splity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells for illustrating layers have been removed. The `Segmenter` model is removed. I have added the model `PredictNext`, which is a text generating model.  \n",
    "\n",
    "A note on language model and the objective to \"given a start symbol, produce a variety of sentences that terminate with a stop symbol\" (part 1 of assignment 2): as designed here, a trained model, would given the same first-token (e.g. start-of-sentence token) always generate the *same* sequence *if we not added some randomness to the text generation*. The solution for doing this here is to define the initial hidden state and cell state of the LSTM by random numbers in text generation (while by zeros in training). Thus, the `PredictNext` model has a method (`initHidden`) which outputs an inital hidden state and an intial cell state by random number or zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB. New cell defining a text generator (using code from previous model definition as basis)\n",
    "\n",
    "class PredictNext(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_dim):\n",
    "        super(PredictNext, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden = hidden_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_size, 0)\n",
    "        self.lstm = nn.LSTM(self.emb_size, self.hidden, batch_first=True)\n",
    "        self.classifier = nn.Linear(self.hidden, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(1) # MB correct dimension?\n",
    "\n",
    "    def forward(self, previous, h_c_states): # M.B. removed lengths\n",
    "        \n",
    "        bsz = previous.shape[0]\n",
    "        \n",
    "        emb_previous = self.emb(previous)\n",
    "        output, (hidden, cell) = self.lstm(emb_previous, h_c_states)\n",
    "        classification_over_vocabulary = self.classifier(hidden.reshape(bsz, self.hidden)) # MB length of input and output is 1\n",
    "        classification_over_vocabulary = self.softmax(classification_over_vocabulary)\n",
    "        next_one = classification_over_vocabulary.argmax(1).unsqueeze(1)\n",
    "        \n",
    "        return next_one, classification_over_vocabulary, (hidden, cell)\n",
    "    \n",
    "    def initHidden(self, batchsize, zero = True, distrib_low = -2, distrib_high = 2):\n",
    "        \"\"\" MB. For initialization of hidden state and cell state of LSTMs. There are \n",
    "            two options here: zero initialization and random initialization. \n",
    "            Random initialization ranges from `distrib_low` to `distrib_high`. Here, \n",
    "            the `Uniform` function from `torch.distributions.uniform` is used for this.\n",
    "            Given some experimenting `torch.rand` (which is based on a uniform \n",
    "            distribution from 0 to 1) gave only minimal variation in sentence generation\n",
    "            (see below); thus, the approach chosen here. However, the default values \n",
    "            selected here are arbitrary and have little variation other than \n",
    "            `text_generator`(below) should yield (substantially) different sequences \n",
    "            when called.\n",
    "        \"\"\"\n",
    "        \n",
    "        if zero:\n",
    "            init_hidden = torch.zeros(1, batchsize, self.hidden, device = gpu_device) # for unstacked lstms; see https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "            init_cell = torch.zeros(1, batchsize, self.hidden, device = gpu_device)\n",
    "#         else:\n",
    "#             init_hidden = torch.rand(1, batchsize, self.hidden, device = gpu_device) # for unstacked lstms; see https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "#             init_cell = torch.rand(1, batchsize, self.hidden, device = gpu_device)\n",
    "        else:\n",
    "            init_hidden = Uniform(distrib_low, distrib_high).sample([1, batchsize, self.hidden]).to(gpu_device)\n",
    "            init_cell = Uniform(distrib_low, distrib_high).sample([1, batchsize, self.hidden]).to(gpu_device)\n",
    "        \n",
    "        return init_hidden, init_cell  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is in large parts the same as in the original. However, modifications have been made to fit the new model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB modified: \n",
    "#    one variable per line; \n",
    "#    variable for LSTM hidden dimension; \n",
    "\n",
    "def train(X, \n",
    "          lengths, \n",
    "          y, \n",
    "          vocab_size, \n",
    "          emb_size, \n",
    "          lstm_hidden_dim, \n",
    "          batch_size, \n",
    "          epochs, \n",
    "          device,\n",
    "          model=None): \n",
    "    \n",
    "    b = Batcher(X, lengths, y, device, batch_size=batch_size, max_iter=epochs)\n",
    "    \n",
    "    if not model:\n",
    "        m = PredictNext(vocab_size, emb_size, lstm_hidden_dim).to(device) \n",
    "    else:\n",
    "        m = model\n",
    "        \n",
    "    loss = nn.NLLLoss(ignore_index=-1) # MB note-to-self: ignore index=-1\n",
    "    optimizer = optim.Adam(m.parameters(), lr=0.005)\n",
    "    epoch = 0\n",
    "    \n",
    "    \n",
    "    for split in b:\n",
    "        tot_loss = 0\n",
    "        for batch in split:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            sentence = batch[0]\n",
    "            lengths = batch[1]\n",
    "            bsz = sentence.shape[0]\n",
    "            seq_len = sentence.shape[1]\n",
    "\n",
    "            total_batch_loss = 0\n",
    "            \n",
    "            init_hidden, init_cell = m.initHidden(bsz)\n",
    "            h_c_states = (init_hidden, init_cell)\n",
    "\n",
    "            loss_o2 = 0\n",
    "            the_who = sentence[:, 0].unsqueeze(1)\n",
    "\n",
    "            for i in range(1, seq_len-1):\n",
    "\n",
    "                the_who, my_generation, h_c_states = m(the_who, h_c_states)\n",
    "\n",
    "                target = sentence[:, i]\n",
    "\n",
    "                # MB a little detour for padding's sake... \n",
    "                pad_idx = char_index[0] # MB. this the index for padding in sentences\n",
    "                target = torch.where(target == pad_idx, -1, target) # MB. https://pytorch.org/docs/stable/generated/torch.where.html\n",
    "                # MB. -1 is the padding of targets and the ignored index in loss \n",
    "\n",
    "                # MB. PS. I now realize that I, like the segmentation model, could have used\n",
    "                # the `train_lengths_tensor` for \"ignoring\" the padding in loss calculation, \n",
    "                # but I am already past deadline and do not implement that solution here, now. \n",
    "\n",
    "                loss_for_this_prediction = loss(my_generation, target)\n",
    "\n",
    "                loss_o2 += loss_for_this_prediction\n",
    "\n",
    "            total_batch_loss += loss_o2\n",
    "            \n",
    "            tot_loss += total_batch_loss\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(\"Total loss in epoch {} is {}.\".format(epoch+1, tot_loss)) # MB added +1\n",
    "        epoch += 1\n",
    "    return m\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 1 is 57142.16796875.\n",
      "Total loss in epoch 2 is 54735.671875.\n",
      "Total loss in epoch 3 is 54541.796875.\n",
      "Total loss in epoch 4 is 54247.13671875.\n",
      "Total loss in epoch 5 is 54507.0625.\n",
      "Total loss in epoch 6 is 54604.96875.\n",
      "Total loss in epoch 7 is 53613.05859375.\n",
      "Total loss in epoch 8 is 53991.3671875.\n",
      "Total loss in epoch 9 is 53163.62890625.\n",
      "Total loss in epoch 10 is 53773.68359375.\n"
     ]
    }
   ],
   "source": [
    "# MB modification: one parameter per line (easier to read)\n",
    "model = train(X = train_X_tensor, \n",
    "                  lengths = train_lengths_tensor, \n",
    "                  y = train_y_tensor, \n",
    "                  vocab_size = len(int_index), \n",
    "                  emb_size = 200, \n",
    "                  lstm_hidden_dim = 150, \n",
    "                  batch_size = 50, \n",
    "                  epochs = 10, \n",
    "                  device = gpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function `text_generator` has been defined to generate text from a trained model. Note here (as mentioned above) that initialization of hidden and cell state are set by random numbers. One each call, `text_generator` generate different sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB. New cell.\n",
    "def text_generator(model, prime_token = sos, max_length = 180):\n",
    "    model.eval()\n",
    "\n",
    "    rolling_stone = torch.tensor([[char_index[prime_token]]]).to(gpu_device)\n",
    "    \n",
    "    hidden_cell_states = model.initHidden(1, zero = False)\n",
    "    # MB. We need some variation to the loop in order to produce variable sentences. \n",
    "    # To use random initialization for the initial hidden and cell state of the LSTM  \n",
    "    # is my suggestion for solving that. \n",
    "    \n",
    "    length_of_generation = 0\n",
    "    ex_nihilo = []\n",
    "    end_of_sentence = False\n",
    "    length_of_generation = 0\n",
    "    \n",
    "    while end_of_sentence == False and length_of_generation < max_length:\n",
    "        rolling_stone, just_ignore_this, hidden_cell_states = model(rolling_stone, hidden_cell_states)\n",
    "        rs_as_string = str(int_index[rolling_stone.flatten()])\n",
    "        if rs_as_string == eos:\n",
    "            end_of_sentence = True\n",
    "        else:\n",
    "            ex_nihilo.append(rs_as_string)\n",
    "            length_of_generation += 1\n",
    "\n",
    "    return \"\".join(ex_nihilo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根10，，，，，，，，，，，，。。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MB. New cell. \n",
    "text_generator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'這9，，，，，，，，，，。。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... being different from:\n",
    "text_generator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
