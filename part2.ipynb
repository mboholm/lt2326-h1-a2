{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data, padding (based on 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB collected all libraries at one place\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M.B. New\n",
    "gpu_device = \"cuda:2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M.B. ADDED EOS AND SOS\n",
    "\n",
    "sos = \"#\"\n",
    "eos = \"!\"\n",
    "# Neither \"#\" nor \"!\" seems to be in the original data\n",
    "\n",
    "def read_chinese_data(inputfilename):\n",
    "    with open(inputfilename, \"r\") as inputfile:\n",
    "        sentences = []\n",
    "        collection_words = []\n",
    "        collection_labels = []\n",
    "        for line in inputfile:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            columns = line.split()\n",
    "            #print(words)\n",
    "            if columns == []:\n",
    "                collection_words = [sos] + collection_words + [eos] \n",
    "                collection_labels = [1] + collection_labels + [1]\n",
    "\n",
    "                sentences.append((''.join(collection_words), collection_labels))\n",
    "                collection_words = []\n",
    "                collection_labels = []\n",
    "                continue\n",
    "            \n",
    "            collection_words.append(columns[1])\n",
    "            collection_labels += [1] + ([0] * (len(columns[1]) - 1))\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_chinese_data('/scratch/lt2316-h20-resources/zh_gsd-ud-train.conllu')\n",
    "# train_sentences[0] # MB added this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = read_chinese_data('/scratch/lt2316-h20-resources/zh_gsd-ud-test.conllu')\n",
    "# test_sentences[0] # MB added this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_chars(sentences):\n",
    "    megasentence = ''.join(sentences)\n",
    "    char_list = set()\n",
    "    for c in megasentence:\n",
    "        char_list.add(c)\n",
    "    char_list = [0] + list(char_list)\n",
    "    return char_list, {char_list[x]:x for x in range(len(char_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_index, char_index = index_chars([x[0] for x in train_sentences + test_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(sentence, index):\n",
    "    return [index[x] for x in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_lengths(sentences, max_length, padding=0):\n",
    "    return [x + ([padding] * (max_length - len(x))) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x, device=\"cpu\"):\n",
    "    converted = [(convert_sentence(x1[0], char_index), x1[1]) for x1 in x]\n",
    "    X, y = zip(*converted)\n",
    "    lengths = [len(x2) for x2 in X]\n",
    "    padded_X = pad_lengths(X, max(lengths))\n",
    "    Xt = torch.LongTensor(padded_X).to(device)\n",
    "    padded_y = pad_lengths(y, max(lengths), padding=-1)\n",
    "    yt = torch.LongTensor(padded_y).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    return Xt, lengths_t, yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor, train_lengths_tensor, train_y_tensor = create_dataset(train_sentences, gpu_device)\n",
    "test_X_tensor, test_lengths_tensor, test_y_tensor = create_dataset(test_sentences, gpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packing the sequences for RNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "testtensor = torch.randn((10,100,200))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testlengths = torch.randint(1, 100, (10,))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testlengths.size(), testlengths"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "packed = pack_padded_sequence(testtensor, testlengths, batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testtensor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "packed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(packed.batch_sizes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "unpacked = pad_packed_sequence(packed, batch_first=True, total_length=100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "unpacked"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "unpacked[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "unpacked[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching (based on 1.0, 1.1, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, X, lengths, y, device, batch_size=50, max_iter=None):\n",
    "        self.X = X\n",
    "        self.lengths = lengths # We need the lengths to efficiently use the padding.\n",
    "        self.y = y\n",
    "        self.device = device\n",
    "        self.batch_size=batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.curr_iter = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.curr_iter == self.max_iter:\n",
    "            raise StopIteration\n",
    "        permutation = torch.randperm(self.X.size()[0], device=self.device)\n",
    "        permX = self.X[permutation]\n",
    "        permlengths = self.lengths[permutation]\n",
    "        permy = self.y[permutation]\n",
    "        splitX = torch.split(permX, self.batch_size)\n",
    "        splitlengths = torch.split(permlengths, self.batch_size)\n",
    "        splity = torch.split(permy, self.batch_size)\n",
    "        \n",
    "        self.curr_iter += 1\n",
    "        return zip(splitX, splitlengths, splity)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b = Batcher(train_X_tensor, train_lengths_tensor, train_y_tensor, torch.device('cuda:2'), max_iter=100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testbatching = next(b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testbatching"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testbatch = next(testbatching)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "emb = nn.Embedding(len(int_index), 200, 0).to(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testX, testlengths, testy = testbatch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testembs = emb(testX)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testembs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testembs.size()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testembs.device"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testlstm = nn.LSTM(200, 150, batch_first=True).to(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testembspadded = pack_padded_sequence(testembs, testlengths.to(\"cpu\"), batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testoutput, teststate = testlstm(testembspadded)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testoutput"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testunpacked = pad_packed_sequence(testoutput, batch_first=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testunpacked[0].size()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testsigm = nn.Sigmoid().to(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testoutput2 = testsigm(testunpacked[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testoutput2.size()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testlin = nn.Linear(150, 2).to(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testoutput3 = testlin(testoutput2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testoutput3.size()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testsoft = nn.LogSoftmax(2).to(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testoutput4 = testsoft(testoutput3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testoutput4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testy_short = testy[:, :max(testlengths)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testy_short"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testy_short.size()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "max(testlengths)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testpermuted = testoutput4.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testpermuted"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nllloss = nn.NLLLoss(ignore_index=-1).to(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nllloss(testpermuted, testy_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB added variable for hidden dim, otherwise as before\n",
    "class Segmenter(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden = hidden_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_size, 0)\n",
    "        self.lstm = nn.LSTM(self.emb_size, self.hidden, batch_first=True)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.lin = nn.Linear(self.hidden, 2)\n",
    "        self.softmax = nn.LogSoftmax(2)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        embs = self.emb(x)\n",
    "        packed = pack_padded_sequence(embs, lengths.to(\"cpu\"), batch_first=True, enforce_sorted=False)\n",
    "        output1, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(output1, batch_first=True)\n",
    "        output2 = self.sig1(unpacked)\n",
    "        output3 = self.lin(output2)\n",
    "        return self.softmax(output3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M.B. NEW usng code from previous model defingition\n",
    "\n",
    "class PredictNext(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_dim):\n",
    "        super(PredictNext, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden = hidden_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_size, 0)\n",
    "        self.lstm = nn.LSTM(self.emb_size, self.hidden, batch_first=True)\n",
    "        self.classifier = nn.Linear(self.hidden, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(1) # MB correct dimension?\n",
    "\n",
    "    def forward(self, previous, h_c_states): # M.B. removed lengths\n",
    "        \n",
    "        bsz = previous.shape[0]\n",
    "        \n",
    "        emb_previous = self.emb(previous)\n",
    "        #packed = pack_padded_sequence(embs, lengths.to(\"cpu\"), batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell) = self.lstm(emb_previous, h_c_states)\n",
    "        \n",
    "        classification_over_vocabulary = self.classifier(hidden.reshape(bsz, self.hidden)) # MB length of input and output is 1\n",
    "        \n",
    "        classification_over_vocabulary = self.softmax(classification_over_vocabulary)\n",
    "        \n",
    "        next_one = classification_over_vocabulary.argmax(1).unsqueeze(1)\n",
    "        #print(\"next_one\", next_one)\n",
    "        \n",
    "        return next_one, classification_over_vocabulary, (hidden, cell)\n",
    "    \n",
    "    def initHidden(self, batchsize, zero = True):\n",
    "        \n",
    "        if zero:\n",
    "            init_hidden = torch.zeros(1, batchsize, self.hidden, device = gpu_device) # for unstacked lstms; see https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "            init_cell = torch.zeros(1, batchsize, self.hidden, device = gpu_device)\n",
    "        else:\n",
    "            init_hidden = torch.rand(1, batchsize, self.hidden, device = gpu_device) # for unstacked lstms; see https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "            init_cell = torch.rand(1, batchsize, self.hidden, device = gpu_device)\n",
    "        \n",
    "        return init_hidden, init_cell           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M.B. NEW\n",
    "\n",
    "class DoubleObjective(nn.Module):\n",
    "    def __init__(self, segmentation_model, text_generator):\n",
    "        super(DoubleObjective, self).__init__()\n",
    "        \n",
    "        self.segmenter = segmentation_model\n",
    "        self.generator = text_generator\n",
    "        \n",
    "    def forward(self, inp, lengths, h_c_states, modus):\n",
    "        \n",
    "        if modus == \"segment\":\n",
    "            segmentation = self.segmenter(inp, lengths)\n",
    "            #print(\"segmentation in model\", segmentation)\n",
    "            return segmentation\n",
    "        \n",
    "        elif modus == \"generate\":\n",
    "            next_one, classification, h_c_state = self.generator(inp, h_c_states)\n",
    "            return next_one, classification, h_c_state\n",
    "        \n",
    "        else:\n",
    "            print(\"ERROR: You have not provided any of the allowed modi; which are 'segment' and 'generate'.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB Modified\n",
    "\n",
    "def train(X, \n",
    "          lengths, \n",
    "          y, \n",
    "          vocab_size, \n",
    "          emb_size, \n",
    "          lstm_hidden_dim, \n",
    "          batch_size, \n",
    "          epochs, \n",
    "          device, \n",
    "          model=None): \n",
    "    \n",
    "    b = Batcher(X, lengths, y, device, batch_size=batch_size, max_iter=epochs)\n",
    "    \n",
    "    if not model:\n",
    "        my_segmenter = Segmenter(vocab_size, emb_size, lstm_hidden_dim).to(device)\n",
    "        my_generator = PredictNext(vocab_size, emb_size, lstm_hidden_dim).to(device) \n",
    "        # MB note: embedding size and hidden dimension of LSTM could have been differentiated\n",
    "        m = DoubleObjective(my_segmenter, my_generator)\n",
    "    else:\n",
    "        m = model\n",
    "        \n",
    "    loss = nn.NLLLoss(ignore_index=-1) # MB note-to-self: ignore index\n",
    "    optimizer = optim.Adam(m.parameters(), lr=0.005)\n",
    "    epoch = 0\n",
    "    \n",
    "    for split in b:\n",
    "        tot_loss = 0\n",
    "        for batch in split:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            sentence = batch[0]\n",
    "            lengths = batch[1]\n",
    "            bsz = sentence.shape[0]\n",
    "            seq_len = sentence.shape[1]\n",
    "\n",
    "            # Objective: segmentation\n",
    "            #print(sentence)\n",
    "            segmentation = m(sentence, lengths, None, modus = \"segment\")\n",
    "            #print(segmentation)\n",
    "            trgs = batch[2]\n",
    "            loss_o1 = loss(segmentation.permute(0,2,1), trgs[:, :max(lengths)])\n",
    "            \n",
    "            # Objective: generation\n",
    "            init_hidden, init_cell = m.generator.initHidden(bsz)\n",
    "            h_c_states = (init_hidden, init_cell)\n",
    "            \n",
    "            #my_generation = torch.zeros(bsz, seq_len-1, self.generator.vocab_size).to(gpu_device) # seq_len -1 ?\n",
    "            #the_who = sentence[:, 0].unsqueeze(1) # a column of start symbols; unsqueezed\n",
    "            \n",
    "            loss_o2 = 0\n",
    "            \n",
    "            for i in range(seq_len-1):\n",
    "                the_who = sentence[:, i].unsqueeze(1)\n",
    "                #print(\"the_who\", i, the_who)\n",
    "                the_who, my_generation, h_c_states = m(the_who, None, h_c_states, modus = \"generate\")\n",
    "                #print(\"the_who\", i, the_who)\n",
    "                \n",
    "                target = sentence[:, i]\n",
    "                loss_for_this_prediction = loss(my_generation, target)\n",
    "                \n",
    "                loss_o2 += loss_for_this_prediction\n",
    "                \n",
    "#             loss_o2 = loss(sentence_generations.reshape(bsz * (seq_len-1), m.generator.vocab_size), \n",
    "#                            sentence[:, 1:].flatten())\n",
    "            \n",
    "            total_batch_loss = loss_o1 + loss_o2\n",
    "            \n",
    "            tot_loss += total_batch_loss\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Total loss in epoch {} is {}.\".format(epoch, tot_loss))\n",
    "        epoch += 1\n",
    "    return m\n",
    "\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MB modification: one parameter per line (easier to read)\n",
    "model = train(X = train_X_tensor, \n",
    "              lengths = train_lengths_tensor, \n",
    "              y = train_y_tensor, \n",
    "              vocab_size = len(int_index), \n",
    "              emb_size = 200, \n",
    "              lstm_hidden_dim = 150, \n",
    "              batch_size = 50, \n",
    "              epochs = 1, \n",
    "              device = gpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB New\n",
    "def text_generator(prime_token = sos, max_length = 180, detach_me = True):\n",
    "    model.eval()\n",
    "    \n",
    "    start_me_up = torch.tensor([[char_index[prime_token]]]).to(gpu_device)\n",
    "    print(start_me_up.shape)\n",
    "    \n",
    "    hidden_cell_states = model.generator.initHidden(1)\n",
    "    \n",
    "    length_of_generation = 0\n",
    "    ex_nihilo = []\n",
    "    \n",
    "    while output != eos or length_of_generation < max_length:\n",
    "        \n",
    "        \n",
    "    \n",
    "    gen = model(start_me_up, None, hidden_cell_states, generate_only=True)\n",
    "    \n",
    "    print(gen)\n",
    "    \n",
    "text_generator()    \n",
    "\n",
    "\n",
    "next_one, classification_over_vocabulary, (hidden, cell)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rawpredictions = model(test_X_tensor, test_lengths_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawpredictions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawpredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.log2(0.9), math.log2(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(rawpredictions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lengths_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectpreds = []\n",
    "collecty = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test_X_tensor.size(0)):\n",
    "    collectpreds.append(predictions[i][:test_lengths_tensor[i]])\n",
    "    collecty.append(test_y_tensor[i][:test_lengths_tensor[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collecty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds = torch.cat(collectpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = torch.cat(collecty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = classes.float()\n",
    "allpreds = allpreds.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = sum(classes * allpreds)\n",
    "fp = sum(classes * (~allpreds.bool()).float())\n",
    "tn = sum((~classes.bool()).float() * (~allpreds.bool()).float())\n",
    "fn = sum((~classes.bool()).float() * allpreds)\n",
    "\n",
    "tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp / (tp + fn)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / (tp + fp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = (2 * recall * precision) / (recall + precision)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
