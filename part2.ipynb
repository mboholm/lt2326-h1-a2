{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**: Max Boholm (gusbohom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB collected all libraries at one place\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.uniform import Uniform\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB. New cell\n",
    "gpu_device = \"cuda:2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data, padding (based on 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start-of-sentence (sos) and end-of-sentence (eos) tokens have been added to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB added code for adding start-of-sentence (sos) and end-of-sentence (eos) tokens\n",
    "\n",
    "sos = \"#\"\n",
    "eos = \"!\"\n",
    "# MB. Neither \"#\" nor \"!\" seems to be in the original data\n",
    "\n",
    "def read_chinese_data(inputfilename):\n",
    "    with open(inputfilename, \"r\") as inputfile:\n",
    "        sentences = []\n",
    "        collection_words = []\n",
    "        collection_labels = []\n",
    "        for line in inputfile:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            columns = line.split()\n",
    "            if columns == []:\n",
    "                collection_words = [sos] + collection_words + [eos] # MB modified the iteration here\n",
    "                collection_labels = [1] + collection_labels + [1]   # ... and here\n",
    "\n",
    "                sentences.append((''.join(collection_words), collection_labels))\n",
    "                collection_words = []\n",
    "                collection_labels = []\n",
    "                continue\n",
    "            \n",
    "            collection_words.append(columns[1])\n",
    "            collection_labels += [1] + ([0] * (len(columns[1]) - 1))\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_chinese_data('/scratch/lt2316-h20-resources/zh_gsd-ud-train.conllu')\n",
    "# train_sentences[0] # MB added this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = read_chinese_data('/scratch/lt2316-h20-resources/zh_gsd-ud-test.conllu')\n",
    "# test_sentences[0] # MB added this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_chars(sentences):\n",
    "    megasentence = ''.join(sentences)\n",
    "    char_list = set()\n",
    "    for c in megasentence:\n",
    "        char_list.add(c)\n",
    "    char_list = [0] + list(char_list)\n",
    "    return char_list, {char_list[x]:x for x in range(len(char_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_index, char_index = index_chars([x[0] for x in train_sentences + test_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(sentence, index):\n",
    "    return [index[x] for x in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_lengths(sentences, max_length, padding=0):\n",
    "    return [x + ([padding] * (max_length - len(x))) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x, device=\"cpu\"):\n",
    "    converted = [(convert_sentence(x1[0], char_index), x1[1]) for x1 in x]\n",
    "    X, y = zip(*converted)\n",
    "    lengths = [len(x2) for x2 in X]\n",
    "    padded_X = pad_lengths(X, max(lengths))\n",
    "    Xt = torch.LongTensor(padded_X).to(device)\n",
    "    padded_y = pad_lengths(y, max(lengths), padding=-1)\n",
    "    yt = torch.LongTensor(padded_y).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    return Xt, lengths_t, yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor, train_lengths_tensor, train_y_tensor = create_dataset(train_sentences, gpu_device)\n",
    "test_X_tensor, test_lengths_tensor, test_y_tensor = create_dataset(test_sentences, gpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Packing the sequences for RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells for illstrating structure of data and performance of `pack_padded_sequence` and `pad_packed_sequence` have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batching (based on 1.0, 1.1, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells for illustrating `Batcher` have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, X, lengths, y, device, batch_size=50, max_iter=None):\n",
    "        self.X = X\n",
    "        self.lengths = lengths # We need the lengths to efficiently use the padding.\n",
    "        self.y = y\n",
    "        self.device = device\n",
    "        self.batch_size=batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.curr_iter = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.curr_iter == self.max_iter:\n",
    "            raise StopIteration\n",
    "        permutation = torch.randperm(self.X.size()[0], device=self.device)\n",
    "        permX = self.X[permutation]\n",
    "        permlengths = self.lengths[permutation]\n",
    "        permy = self.y[permutation]\n",
    "        splitX = torch.split(permX, self.batch_size)\n",
    "        splitlengths = torch.split(permlengths, self.batch_size)\n",
    "        splity = torch.split(permy, self.batch_size)\n",
    "        \n",
    "        self.curr_iter += 1\n",
    "        return zip(splitX, splitlengths, splity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells for illustrating layers have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB added variable for hidden dimension of LSTM, otherwise as before\n",
    "class Segmenter(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden = hidden_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_size, 0)\n",
    "        self.lstm = nn.LSTM(self.emb_size, self.hidden, batch_first=True)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.lin = nn.Linear(self.hidden, 2)\n",
    "        self.softmax = nn.LogSoftmax(2)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        embs = self.emb(x)\n",
    "        packed = pack_padded_sequence(embs, lengths.to(\"cpu\"), batch_first=True, enforce_sorted=False)\n",
    "        output1, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(output1, batch_first=True)\n",
    "        output2 = self.sig1(unpacked)\n",
    "        output3 = self.lin(output2)\n",
    "        return self.softmax(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB. New cell defining a text generator (using code from previous model definition as basis)\n",
    "\n",
    "class PredictNext(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_dim):\n",
    "        super(PredictNext, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden = hidden_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_size, 0)\n",
    "        self.lstm = nn.LSTM(self.emb_size, self.hidden, batch_first=True)\n",
    "        self.classifier = nn.Linear(self.hidden, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(1) # MB correct dimension?\n",
    "\n",
    "    def forward(self, previous, h_c_states): # M.B. removed lengths\n",
    "        \n",
    "        bsz = previous.shape[0]\n",
    "        \n",
    "        emb_previous = self.emb(previous)\n",
    "        output, (hidden, cell) = self.lstm(emb_previous, h_c_states)\n",
    "        classification_over_vocabulary = self.classifier(hidden.reshape(bsz, self.hidden)) # MB length of input and output is 1\n",
    "        classification_over_vocabulary = self.softmax(classification_over_vocabulary)\n",
    "        next_one = classification_over_vocabulary.argmax(1).unsqueeze(1)\n",
    "        \n",
    "        return next_one, classification_over_vocabulary, (hidden, cell)\n",
    "    \n",
    "    def initHidden(self, batchsize, zero = True, distrib_low = -2, distrib_high = 2):\n",
    "        \"\"\" MB. For initialization of hidden state and cell state of LSTMs. There are \n",
    "            two options here: zero initialization and random initialization. \n",
    "            Random initialization ranges from `distrib_low` to `distrib_high`. Here, \n",
    "            the `Uniform` function from `torch.distributions.uniform` is used for this.\n",
    "            Given some experimenting `torch.rand` (which is based on a uniform \n",
    "            distribution from 0 to 1) gave only minimal variation in sentence generation\n",
    "            (see below); thus, the approach chosen here. However, the default values \n",
    "            selected here are arbitrary and have little variation other than \n",
    "            `text_generator`(below) should yield (substantially) different sequences \n",
    "            when called.\n",
    "        \"\"\"\n",
    "        \n",
    "        if zero:\n",
    "            init_hidden = torch.zeros(1, batchsize, self.hidden, device = gpu_device) # for unstacked lstms; see https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "            init_cell = torch.zeros(1, batchsize, self.hidden, device = gpu_device)\n",
    "#         else:\n",
    "#             init_hidden = torch.rand(1, batchsize, self.hidden, device = gpu_device) # for unstacked lstms; see https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "#             init_cell = torch.rand(1, batchsize, self.hidden, device = gpu_device)\n",
    "        else:\n",
    "            init_hidden = Uniform(distrib_low, distrib_high).sample([1, batchsize, self.hidden]).to(gpu_device)\n",
    "            init_cell = Uniform(distrib_low, distrib_high).sample([1, batchsize, self.hidden]).to(gpu_device)\n",
    "        \n",
    "        return init_hidden, init_cell  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB. New cell defining a double objective model: segement and text generation\n",
    "\n",
    "class DoubleObjective(nn.Module):\n",
    "    def __init__(self, segmentation_model, text_generator):\n",
    "        super(DoubleObjective, self).__init__()\n",
    "        \n",
    "        self.segmenter = segmentation_model\n",
    "        self.generator = text_generator\n",
    "        \n",
    "    def forward(self, inp, lengths, h_c_states, modus):\n",
    "        \n",
    "        if modus == \"segment\":\n",
    "            segmentation = self.segmenter(inp, lengths)\n",
    "            return segmentation\n",
    "        \n",
    "        elif modus == \"generate\":\n",
    "            next_one, classification, h_c_state = self.generator(inp, h_c_states)\n",
    "            return next_one, classification, h_c_state\n",
    "        \n",
    "        else:\n",
    "            print(\"ERROR: You have not provided any of the allowed modi; which are 'segment' and 'generate'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB modified: \n",
    "#    one variable per line; \n",
    "#    variable for LSTM hidden dimension; \n",
    "#    adjustments to fit DoubleObjective model\n",
    "\n",
    "def train(X, \n",
    "          lengths, \n",
    "          y, \n",
    "          vocab_size, \n",
    "          emb_size, \n",
    "          lstm_hidden_dim, \n",
    "          batch_size, \n",
    "          epochs, \n",
    "          device,\n",
    "          segment=True,\n",
    "          generate=True,\n",
    "          model=None): \n",
    "    \n",
    "    b = Batcher(X, lengths, y, device, batch_size=batch_size, max_iter=epochs)\n",
    "    \n",
    "    if not model:\n",
    "        my_segmenter = Segmenter(vocab_size, emb_size, lstm_hidden_dim).to(device)\n",
    "        my_generator = PredictNext(vocab_size, emb_size, lstm_hidden_dim).to(device) \n",
    "        # MB note: embedding size and hidden dimension of LSTM could have been differentiated\n",
    "        m = DoubleObjective(my_segmenter, my_generator)\n",
    "    else:\n",
    "        m = model\n",
    "        \n",
    "    loss = nn.NLLLoss(ignore_index=-1) # MB note-to-self: ignore index=-1\n",
    "    optimizer = optim.Adam(m.parameters(), lr=0.005)\n",
    "    epoch = 0\n",
    "    \n",
    "    my_losses = []\n",
    "    \n",
    "    for split in b:\n",
    "        tot_loss = 0\n",
    "        for batch in split:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            sentence = batch[0]\n",
    "            lengths = batch[1]\n",
    "            bsz = sentence.shape[0]\n",
    "            seq_len = sentence.shape[1]\n",
    "\n",
    "            total_batch_loss = 0\n",
    "            \n",
    "            if segment: # MB. Objective 1: segmentation\n",
    "                segmentation = m(sentence, lengths, None, modus = \"segment\")\n",
    "                trgs = batch[2]\n",
    "                loss_o1 = loss(segmentation.permute(0,2,1), trgs[:, :max(lengths)])\n",
    "                total_batch_loss += loss_o1\n",
    "            \n",
    "            if generate: # MB. Objective 2: generation\n",
    "                init_hidden, init_cell = m.generator.initHidden(bsz)\n",
    "                h_c_states = (init_hidden, init_cell)\n",
    "\n",
    "                loss_o2 = 0\n",
    "                the_who = sentence[:, 0].unsqueeze(1)\n",
    "\n",
    "                for i in range(1, seq_len-1):\n",
    "\n",
    "                    the_who, my_generation, h_c_states = m(the_who, None, h_c_states, modus = \"generate\")\n",
    "\n",
    "                    target = sentence[:, i]\n",
    "\n",
    "                    # MB a little detour for padding's sake... \n",
    "                    pad_idx = char_index[0] # MB. this the index for padding in sentences\n",
    "                    target = torch.where(target == pad_idx, -1, target) # MB. https://pytorch.org/docs/stable/generated/torch.where.html\n",
    "                    # MB. -1 is the padding of targets and the ignored index in loss \n",
    "\n",
    "                    loss_for_this_prediction = loss(my_generation, target)\n",
    "\n",
    "                    loss_o2 += loss_for_this_prediction\n",
    "                    \n",
    "                total_batch_loss += loss_o2\n",
    "            \n",
    "            tot_loss += total_batch_loss\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        my_losses.append(tot_loss.item())\n",
    "        print(\"Total loss in epoch {} is {}.\".format(epoch+1, tot_loss)) # MB added +1\n",
    "        epoch += 1\n",
    "    return m, my_losses\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MB modification: one parameter per line (easier to read)\n",
    "dual_model, dual_loss_array = train(X = train_X_tensor, \n",
    "                                  lengths = train_lengths_tensor, \n",
    "                                  y = train_y_tensor, \n",
    "                                  vocab_size = len(int_index), \n",
    "                                  emb_size = 200, \n",
    "                                  lstm_hidden_dim = 150, \n",
    "                                  batch_size = 50, \n",
    "                                  epochs = 10, \n",
    "                                  device = gpu_device, \n",
    "                                  segment = True, \n",
    "                                  generate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB. New cell.\n",
    "def text_generator(model, prime_token = sos, max_length = 180):\n",
    "    model.eval()\n",
    "\n",
    "    rolling_stone = torch.tensor([[char_index[prime_token]]]).to(gpu_device)\n",
    "    \n",
    "    hidden_cell_states = model.generator.initHidden(1, zero = False)\n",
    "    # MB. We need some variation to the loop in order to produce variable sentences. \n",
    "    # To use random initialization for the initial hidden and cell state of the LSTM  \n",
    "    # is my suggestion for solving that. \n",
    "    \n",
    "    length_of_generation = 0\n",
    "    ex_nihilo = []\n",
    "    end_of_sentence = False\n",
    "    length_of_generation = 0\n",
    "    \n",
    "    while end_of_sentence == False and length_of_generation < max_length:\n",
    "        rolling_stone, just_ignore_this, hidden_cell_states = model(rolling_stone, None, hidden_cell_states, modus = \"generate\")\n",
    "        rs_as_string = str(int_index[rolling_stone.flatten()])\n",
    "        if rs_as_string == eos:\n",
    "            end_of_sentence = True\n",
    "        else:\n",
    "            ex_nihilo.append(rs_as_string)\n",
    "            length_of_generation += 1\n",
    "\n",
    "    return \"\".join(ex_nihilo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MB. New cell. \n",
    "text_generator(dual_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ... being different from:\n",
    "text_generator(dual_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation (Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First**, training the other models for comparison ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MB modification: one parameter per line (easier to read)\n",
    "segment_model, segment_loss_array = train(X = train_X_tensor, \n",
    "                                  lengths = train_lengths_tensor, \n",
    "                                  y = train_y_tensor, \n",
    "                                  vocab_size = len(int_index), \n",
    "                                  emb_size = 200, \n",
    "                                  lstm_hidden_dim = 150, \n",
    "                                  batch_size = 50, \n",
    "                                  epochs = 10, \n",
    "                                  device = gpu_device, \n",
    "                                  segment = True, \n",
    "                                  generate = False) # <---- Note! (MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB modification: one parameter per line (easier to read)\n",
    "gen_model, gen_loss_array = train(X = train_X_tensor, \n",
    "                                  lengths = train_lengths_tensor, \n",
    "                                  y = train_y_tensor, \n",
    "                                  vocab_size = len(int_index), \n",
    "                                  emb_size = 200, \n",
    "                                  lstm_hidden_dim = 150, \n",
    "                                  batch_size = 50, \n",
    "                                  epochs = 10, \n",
    "                                  device = gpu_device, \n",
    "                                  segment = False,      # <---- Note! (MB)\n",
    "                                  generate = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second**, define functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB. New, but uses code from original\n",
    "def measure(array_pred, array_truth):\n",
    "    \n",
    "    classes = array_truth.float()\n",
    "    allpreds = array_pred.float()\n",
    "\n",
    "    tp = sum(classes * allpreds)\n",
    "    fp = sum(classes * (~allpreds.bool()).float())\n",
    "    tn = sum((~classes.bool()).float() * (~allpreds.bool()).float())\n",
    "    fn = sum((~classes.bool()).float() * allpreds)\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = (2 * recall * precision) / (recall + precision)\n",
    "    \n",
    "    return accuracy.item(), f1.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB. New, but uses code from the original.\n",
    "def evaluate_and_compare(model1, model2):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        raw_m1 = model1(test_X_tensor, test_lengths_tensor, None, modus=\"segment\")\n",
    "        raw_m2 = model2(test_X_tensor, test_lengths_tensor, None, modus=\"segment\")\n",
    "    \n",
    "    pred1 = torch.argmax(raw_m1, 2)\n",
    "    pred2 = torch.argmax(raw_m2, 2)\n",
    "    \n",
    "    collectpreds_m1 = []\n",
    "    collectpreds_m2 = []\n",
    "    collecty = []\n",
    "    \n",
    "    for i in range(test_X_tensor.size(0)):\n",
    "        collectpreds_m1.append(pred1[i][:test_lengths_tensor[i]])\n",
    "        collectpreds_m2.append(pred2[i][:test_lengths_tensor[i]])\n",
    "        \n",
    "        collecty.append(test_y_tensor[i][:test_lengths_tensor[i]])\n",
    "    \n",
    "    allpreds_m1 = torch.cat(collectpreds_m1)\n",
    "    allpreds_m2 = torch.cat(collectpreds_m2)\n",
    "    classes = torch.cat(collecty)\n",
    "    \n",
    "    accuracy_m1, f1_m1 = measure(allpreds_m1, classes)\n",
    "    accuracy_m2, f1_m2 = measure(allpreds_m2, classes)\n",
    "    \n",
    "    my_short=4\n",
    "    print(\"EVALUATION:\")\n",
    "    print(f\"\\tAcc.\\tF1\")\n",
    "    print(f\"Model1\\t{round(accuracy_m1, my_short)}\\t{round(f1_m1, my_short)}\")\n",
    "    print(f\"Model2\\t{round(accuracy_m2, my_short)}\\t{round(f1_m2, my_short)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_array):\n",
    "    n_epochs = len(loss_array)\n",
    "    \n",
    "    x = [x for x in range(n_epochs)]\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(x, loss_array)\n",
    "    plt.title(\"Model's loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot_loss(loss_m1, loss_m2):\n",
    "    \n",
    "    assert len(loss_m1) == len(loss_m2), \"Different lengths!\"\n",
    "    \n",
    "    n_epochs = len(loss_m1)\n",
    "    \n",
    "    x = [x for x in range(n_epochs)]\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(x, loss_m1)\n",
    "    plt.plot(x, loss_m2)\n",
    "    plt.title(\"Models' losses\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(something, something):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    function = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    cross_entrophy_loss = function\n",
    "    perplexity = torch.exp(cross_entrophy_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # [pred1]<-->[truth1] : loss1\n",
    "    \n",
    "    # ... mean loss\n",
    "    # ... perplexity = torch.exp(mean loss)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third**, call the functions to get evaluation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_compare(dual_model, segment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(segment_loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot_loss(dual_loss_array, segment_loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Perplexity\n",
    "# https://stackoverflow.com/questions/61988776/how-to-calculate-perplexity-for-a-language-model-using-pytorch\n",
    "\n",
    "\n",
    "losses_for_every_char = []\n",
    "for i in range(test_X_tensor.size(0)):\n",
    "    sentence = test_X_tensor[i]\n",
    "    for j in range(test_lengths_tensor[i]):\n",
    "        my_generation = model()\n",
    "        loss = CrossEntropyLoss(mygeneration, actual_character)\n",
    "        \n",
    "        \n",
    "\n",
    "mean(losses_for_every_char)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells for evaluation from original file has been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bonus Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Identification of the likelihood of characters being initial in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_char = [] # Note: \"true\" data, not predictions\n",
    "collected_y    = []\n",
    "\n",
    "for i in range(test_X_tensor.size(0)):\n",
    "    collected_char.append(test_X_tensor[i][:test_lengths_tensor[i]])\n",
    "    collected_y.append(test_y_tensor[i][:test_lengths_tensor[i]])\n",
    "\n",
    "char_as_one = torch.cat(collected_char)\n",
    "y_as_one = torch.cat(collected_y)\n",
    "\n",
    "char_count = {char_index[char]:0 for char in int_index}\n",
    "as_initial = {char_index[char]:0 for char in int_index}\n",
    "\n",
    "for i in range(char_as_one.shape[0]):\n",
    "    the_character = char_as_one[i].item()\n",
    "    char_count[the_character] += 1\n",
    "    as_initial[the_character] += 1\n",
    "\n",
    "likelihood = {char:0 for char in char_count.keys()}\n",
    "\n",
    "for char in likelihood.keys():\n",
    "    if char_count[char] == 0:\n",
    "        likelihood[char] = 0\n",
    "    else:\n",
    "        likelihood[char] = as_initial[char] / char_count[char]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting dimensions of characters and combine with their \"initiality\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: *Initiality* is here defined as the tendency (liklihood) of a character to be the initial token of a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantics = segment_model.segmenter.emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantics.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_be_matrix = []\n",
    "first_row = [\"Character\", \"Initiality\"]\n",
    "dimensions = [f\"dim{d}\" for d in range(1, semantics.embedding_dim+1)]\n",
    "to_be_matrix.append(first_row+dimensions)\n",
    "#print(to_be_matrix)\n",
    "\n",
    "for char in liklihood.keys():\n",
    "    line = [char]\n",
    "    line.append(liklihood[char])\n",
    "    sem_representation = semantics(torch.tensor(char).to(gpu_device)).tolist()\n",
    "    to_be_matrix.append(line + sem_representation)\n",
    "\n",
    "df = pd.DataFrame(to_be_matrix[1:], columns = to_be_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A version of a function I used for an assignment in LT2222, spring 2021\n",
    "\n",
    "def reduce(matrix, dims=300):\n",
    "    \"\"\" Uses scikit-learn's dimensionality reduction by singular value decomposition (SVD) to reduce a matrix. \n",
    "    See: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\"\"\"\n",
    "\n",
    "    char_column=np.array(matrix[\"Character\"])\n",
    "    init_column=np.array(matrix[\"Initiality\"])    \n",
    "    main_matrix=matrix.drop(labels=\"Character\", axis=1) \n",
    "    main_matrix=matrix.drop(labels=\"Initiality\", axis=1)\n",
    "\n",
    "    #Reduction\n",
    "    svd = TruncatedSVD(n_components=dims, n_iter=7, random_state=42) #values of n_iter and random_state from https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    svd.fit(main_matrix)\n",
    "    reduced_matrix=pd.DataFrame(svd.transform(main_matrix), columns = [\"dim1\", \"dim2\"])\n",
    "\n",
    "    #Return the labels\n",
    "    reduced_matrix.insert(loc=0, column=\"Initiality\", value=init_column)\n",
    "    reduced_matrix.insert(loc=0, column=\"Character\", value=char_column)\n",
    "\n",
    "    return reduced_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = reduce(df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_column(row):\n",
    "    if row[\"Initiality\"] > 0.9:\n",
    "        init_cls = \"High\"\n",
    "    elif row[\"Initiality\"] < 0.1:\n",
    "        init_cls = \"Low\"\n",
    "    else:\n",
    "        init_cls = \"Intermediate\"\n",
    "    return init_cls\n",
    "\n",
    "reduced_df[\"Init_Cls\"] = df.apply(write_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there any correlation of Initiality and Dimension 1 or Dimension 2? \n",
    "**No**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(reduced_df[\"Initiality\"], reduced_df[\"dim1\"])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(reduced_df[\"Initiality\"], reduced_df[\"dim2\"])[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main inspiration from:\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/scatter_with_legend.html\n",
    "\n",
    "#label_color_pairs=[(\"High\", \"red\"), (\"Low\", \"green\"), (\"Intermediate\", \"blue\")]\n",
    "label_color_pairs=[(\"High\", \"red\"), (\"Low\", \"green\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "for label_color in label_color_pairs:\n",
    "    x = reduced_df.loc[reduced_df[\"Init_Cls\"] == label_color[0]][\"dim1\"] \n",
    "    y = reduced_df.loc[reduced_df[\"Init_Cls\"] == label_color[0]][\"dim2\"] \n",
    "    \n",
    "    ax.scatter(x, y, c = label_color[1], label = label_color[0], alpha = 0.5, edgecolors = \"none\") #the setting of alpha and edgecolor \"softens\" the dots somewhat, which is preferable in a crowded plot like this\n",
    "\n",
    "ax.set_xlabel('dim1')\n",
    "ax.set_ylabel('dim2')\n",
    "ax.set_title(\"Plotting Reduced Dimension 1 and Dimension 2 for characters having 'High' or 'Low' Initiality.\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot confirms the correlation. The dimensionality reduced embeddings do not indicate that characters that are more likely to appear at the beginning of a Chinese word are also more similar to one another than characters more likely to appear in the middle or end. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
